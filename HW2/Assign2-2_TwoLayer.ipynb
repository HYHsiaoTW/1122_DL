{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16babb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21f7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, txt_file, transform=None):\n",
    "        self.img_files, self.labels = self.load_img_files(txt_file)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_files[idx])\n",
    "        \n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image {self.img_files[idx]} not found\")\n",
    "        \n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert RGB to greyscale\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        gray_img = np.expand_dims(gray_img, axis=2)  # Add channel dimension\n",
    "\n",
    "        # Stack RGB and greyscale images along the channel dimension\n",
    "        img = np.concatenate((img, gray_img), axis=2)\n",
    "\n",
    "        img = Image.fromarray(img)  # Convert numpy array to PIL Image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "    def load_img_files(self, filename):\n",
    "        if not os.path.isfile(filename):\n",
    "            raise FileNotFoundError(f\"{filename} not found\")\n",
    "\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        img_files, labels = [], []\n",
    "\n",
    "        for line in lines:\n",
    "            fn, label = line.strip().split(' ')\n",
    "            img_files.append(fn)\n",
    "            labels.append(int(label))\n",
    "        \n",
    "        return img_files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1468b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3eb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelConvolution(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(MultiChannelConvolution, self).__init__()\n",
    "        \n",
    "        # Three channels (RGB)\n",
    "        self.conv_rgb = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, padding=3)\n",
    "        \n",
    "        # Two channels combinations\n",
    "        self.conv_rg = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv_rb = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv_gb = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=7, padding=3)\n",
    "        \n",
    "        # One channel\n",
    "        self.conv_r = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv_g = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv_b = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv_gray = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # ResNet block\n",
    "        self.resnet_block = BasicBlock(in_channels=256, out_channels=256, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the channels\n",
    "        rgb = x[:, :3, :, :]      # RGB channels\n",
    "        gray = x[:, 3:, :, :]     # Greyscale channel\n",
    "        \n",
    "        r = rgb[:, 0:1, :, :]\n",
    "        g = rgb[:, 1:2, :, :]\n",
    "        b = rgb[:, 2:3, :, :]\n",
    "\n",
    "        # Apply convolutions\n",
    "        out_rgb = self.conv_rgb(rgb)\n",
    "        \n",
    "        out_rg = self.conv_rg(torch.cat([r, g], dim=1))\n",
    "        out_rb = self.conv_rb(torch.cat([r, b], dim=1))\n",
    "        out_gb = self.conv_gb(torch.cat([g, b], dim=1))\n",
    "        \n",
    "        out_r = self.conv_r(r)\n",
    "        out_g = self.conv_g(g)\n",
    "        out_b = self.conv_b(b)\n",
    "        out_gray = self.conv_gray(gray)\n",
    "\n",
    "        # Collect all results\n",
    "        conv_out = torch.cat([\n",
    "            out_rgb, out_rg, out_rb, out_gb,\n",
    "            out_r, out_g, out_b, out_gray\n",
    "        ], dim=1)\n",
    "        conv_out = self.bn1(conv_out)\n",
    "        conv_out = self.maxpool(conv_out)\n",
    "        conv_out = torch.relu(conv_out)\n",
    "\n",
    "        # Apply the ResNet block\n",
    "        res_out = self.resnet_block(conv_out)\n",
    "        res_out = self.avgpool(res_out)\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        res_out_flat = res_out.view(res_out.size(0), -1)\n",
    "        \n",
    "        # Apply the classifier\n",
    "        class_out = self.fc(res_out_flat)\n",
    "\n",
    "        return class_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7829f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the latest checkpoint\n",
    "def find_latest_checkpoint(model_name):\n",
    "    checkpoints = [f for f in os.listdir(model_name) if f.startswith(model_name) and f.endswith('.pth')]\n",
    "    if not checkpoints:\n",
    "        return None, 0\n",
    "    checkpoints.sort()\n",
    "    latest_checkpoint = checkpoints[-1]\n",
    "    epoch = int(latest_checkpoint.split('_epoch')[1].split('.')[0])\n",
    "    return os.path.join(model_name, latest_checkpoint), epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0605bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded checkpoint 'TwoLayer/TwoLayer_epoch28.pth' (epoch 28)\n",
      "Epoch 29, start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DL_240319/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.690, Validation Accuracy: 0.513\n",
      "Epoch 30, start\n",
      "Validation Loss: 1.676, Validation Accuracy: 0.542\n",
      "Epoch 31, start\n",
      "Validation Loss: 1.691, Validation Accuracy: 0.531\n",
      "Epoch 32, start\n",
      "Validation Loss: 1.694, Validation Accuracy: 0.540\n",
      "Epoch 33, start\n",
      "Validation Loss: 1.642, Validation Accuracy: 0.538\n",
      "Epoch 34, start\n",
      "Validation Loss: 1.663, Validation Accuracy: 0.549\n",
      "Epoch 35, start\n",
      "Validation Loss: 1.676, Validation Accuracy: 0.538\n",
      "Epoch 36, start\n",
      "Validation Loss: 1.643, Validation Accuracy: 0.547\n",
      "Epoch 37, start\n",
      "Validation Loss: 1.666, Validation Accuracy: 0.551\n",
      "Epoch 38, start\n",
      "Validation Loss: 1.643, Validation Accuracy: 0.564\n",
      "Epoch 39, start\n",
      "Validation Loss: 1.667, Validation Accuracy: 0.533\n",
      "Epoch 40, start\n",
      "Validation Loss: 1.592, Validation Accuracy: 0.571\n",
      "Epoch 41, start\n",
      "Validation Loss: 1.636, Validation Accuracy: 0.547\n",
      "Epoch 42, start\n",
      "Validation Loss: 1.674, Validation Accuracy: 0.531\n",
      "Epoch 43, start\n",
      "Validation Loss: 1.613, Validation Accuracy: 0.549\n",
      "Epoch 44, start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 72\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:    \u001b[38;5;66;03m# print every 10 mini-batches\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     batch_loss_log\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "lr = 0.0002\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.5], std=[0.229, 0.224, 0.225, 0.5])\n",
    "])\n",
    "\n",
    "# Load the training and validation datasets\n",
    "train_dataset = CustomDataset(txt_file='train.txt', transform=transform)\n",
    "val_dataset = CustomDataset(txt_file='val.txt', transform=transform)\n",
    "test_dataset = CustomDataset(txt_file='test.txt', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the ResNet model, loss function, and optimizer\n",
    "num_classes = len(set(train_dataset.labels))  # Number of unique classes in the dataset\n",
    "model = MultiChannelConvolution(num_classes).to(device)\n",
    "model_name = 'TwoLayer'\n",
    "os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "latest_checkpoint, start_epoch = find_latest_checkpoint(model_name)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    model.load_state_dict(torch.load(latest_checkpoint))\n",
    "    print(f\"Loaded checkpoint '{latest_checkpoint}' (epoch {start_epoch})\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create log files\n",
    "batch_loss_log_path = os.path.join(model_name, f'{model_name}-batch_loss_log.txt')\n",
    "batch_loss_log = open(batch_loss_log_path, \"w\")\n",
    "epoch_log_path = os.path.join(model_name, f'{model_name}-epoch_log.txt')\n",
    "epoch_log = open(epoch_log_path, \"w\")\n",
    "test_log_path = os.path.join(model_name, f'{model_name}-test_log.txt')\n",
    "test_log = open(epoch_log_path, \"w\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    optimizer_state_path = latest_checkpoint.replace('.pth', '_optimizer.pth')\n",
    "    if os.path.exists(optimizer_state_path):\n",
    "        optimizer.load_state_dict(torch.load(optimizer_state_path))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f'Epoch {epoch + 1}, start')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            batch_loss_log.write(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item():.3f}\\n')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    model_save_path = os.path.join(model_name, f'{model_name}_epoch{epoch+1:02}.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    epoch_val_acc = correct.double() / len(val_loader.dataset)\n",
    "    epoch_log.write(f'Epoch {epoch + 1}, Validation Loss: {epoch_val_loss:.3f}, Validation Accuracy: {epoch_val_acc:.3f}\\n')\n",
    "    print(f'Validation Loss: {epoch_val_loss:.3f}, Validation Accuracy: {epoch_val_acc:.3f}')\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_test_loss = test_loss / len(test_loader.dataset)\n",
    "epoch_test_acc = correct.double() / len(test_loader.dataset)\n",
    "test_log.write(f'Epoch {num_epochs}, Test Loss: {epoch_test_loss:.3f}, Test Accuracy: {epoch_test_acc:.3f}\\n')\n",
    "print(f'Test Loss: {epoch_test_loss:.3f}, Test Accuracy: {epoch_test_acc:.3f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Close log files\n",
    "batch_loss_log.close()\n",
    "epoch_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de749f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_240319",
   "language": "python",
   "name": "dl_240319"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
