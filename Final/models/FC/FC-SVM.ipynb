{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa4aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### this is for classification in single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5e9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 07:15:51.977776: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-13 07:15:51.977817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-13 07:15:51.978229: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-13 07:15:51.981056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "import random\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec49ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, file_path, path_prefix=\"\"):\n",
    "        self.path_prefix = path_prefix\n",
    "        # full_file_path = path_prefix + file_path\n",
    "        full_file_path = 'condition+classify.txt'\n",
    "        with open(full_file_path, 'r') as file:\n",
    "            data = [line.strip().split() for line in file.readlines()]\n",
    "        \n",
    "        self.cond_label = {cond_label: idx for idx, cond_label in enumerate(set(row[1] for row in data))}\n",
    "        self.files = [(row[0], self.cond_label[row[1]]) for row in data]\n",
    "        random.shuffle(self.files)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, cond_label = self.files[idx]\n",
    "        full_img_path = self.path_prefix + img_path\n",
    "        \n",
    "        img = nib.load(full_img_path).get_fdata()\n",
    "        img = np.float32(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        if img.ndim == 4 and img.shape[-1] == 1:\n",
    "            img = img.squeeze(-1)\n",
    "        img = img.unsqueeze(0)\n",
    "        cond_label = torch.tensor(cond_label, dtype=torch.long)\n",
    "        return img, cond_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4ebbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVM, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten the input\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26de1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, test_data=None, epochs=10):\n",
    "    model.train()\n",
    "    with open('training_log.txt', 'a') as log_file:\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                labels = labels.to(device)\n",
    "                inputs = inputs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                target_labels = 2 * labels - 1\n",
    "                loss = criterion(outputs, target_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            average_loss = total_loss / num_batches\n",
    "            current_utc = datetime.datetime.utcnow()\n",
    "            gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "            current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            log_entry = f'Epoch {epoch+1:03}, Average Loss: {average_loss}, Timestamp: {current_time}\\n'\n",
    "            # Write the log entry to the file\n",
    "            log_file.write(log_entry)\n",
    "            # Test every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0 and test_data is not None:\n",
    "                test_input, test_label = test_data\n",
    "                test_result = test_model(model, test_input, test_label, device)\n",
    "                log_file.write(f\"Test at Epoch {epoch+1:03}: {test_result}\\n\")\n",
    "\n",
    "\n",
    "def test_model(model, test_input, test_label, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure test_input is a tensor and move it to the correct device\n",
    "    if not torch.is_tensor(test_input):\n",
    "        test_input = torch.tensor(test_input, dtype=torch.float, device=device)\n",
    "    else:\n",
    "        test_input = test_input.to(device)\n",
    "\n",
    "    # Ensure test_label is a tensor, add a batch dimension, and move to correct device\n",
    "    if isinstance(test_label, int):\n",
    "        test_label = torch.tensor([test_label], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Perform model inference and get the predicted class\n",
    "        outputs = model(test_input.unsqueeze(0))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Check if the prediction is correct\n",
    "        correct = (predicted == test_label).item()  # Convert the result to Python boolean\n",
    "\n",
    "    return \"Correct\" if correct else \"Incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56ab829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(classifier, path_prefix=\"\", epochs=10, lr=0.001, batch=8):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    dataset_file = f'{classifier}+classify.txt'\n",
    "    \n",
    "    full_dataset = MRIDataset(dataset_file, path_prefix=path_prefix)\n",
    "    num_classes = len(set(full_dataset.cond_label))\n",
    "    grand_results = []\n",
    "    \n",
    "    current_utc = datetime.datetime.utcnow()\n",
    "    gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "    current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start_time = f'Start training at: {current_time}'\n",
    "    print(start_time)\n",
    "    \n",
    "    for i in range(len(full_dataset)):\n",
    "        train_indices = list(range(len(full_dataset)))\n",
    "        train_indices.pop(i)  # Remove the test image index\n",
    "        test_index = i\n",
    "        \n",
    "        train_subset = Subset(full_dataset, train_indices)\n",
    "        test_input, test_label = full_dataset[test_index]\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "        for inputs, labels in train_loader:\n",
    "            input_dim = inputs.shape[2] * inputs.shape[3] * inputs.shape[4]\n",
    "            break\n",
    "            \n",
    "        model = SVM(input_dim)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = nn.HingeEmbeddingLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "        train_model(model, train_loader, criterion, optimizer, device, test_data=full_dataset[test_index], epochs=epochs)\n",
    "        result = test_model(model, test_input, test_label, device)\n",
    "        grand_results.append(result)\n",
    "        \n",
    "        os.rename('training_log.txt', f'training_log+stim_{i+1:03}.txt')\n",
    "\n",
    "    with open(f'{classifier}_final_results.log', 'w') as f:\n",
    "        correct_count = grand_results.count(\"Correct\")\n",
    "        total_tests = len(grand_results)\n",
    "        correct_percentage = (correct_count / total_tests) * 100 if total_tests > 0 else 0\n",
    "        for idx, result in enumerate(grand_results):\n",
    "            f.write(f\"Model {idx+1:03}: Result: {result}\\n\")\n",
    "        f.write(f\"Percentage of Correct Predictions: {correct_percentage:.2f}%\\n\")\n",
    "\n",
    "    print(f\"Percentage of Correct Predictions: {correct_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5672babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Start training at: 2024-06-13 15:15:53\n",
      "Percentage of Correct Predictions: 31.58%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-13 16:20:16\n",
      "Percentage of Correct Predictions: 15.62%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-13 17:13:05\n",
      "Percentage of Correct Predictions: 42.55%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-13 19:09:17\n",
      "Percentage of Correct Predictions: 37.78%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-13 20:52:37\n",
      "Percentage of Correct Predictions: 39.02%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-13 22:18:16\n",
      "Percentage of Correct Predictions: 38.64%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-14 00:00:41\n",
      "Percentage of Correct Predictions: 40.43%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-14 01:57:20\n",
      "Percentage of Correct Predictions: 28.21%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-14 03:16:09\n",
      "Percentage of Correct Predictions: 28.95%\n",
      "Using device: cuda\n",
      "Start training at: 2024-06-14 04:26:20\n",
      "Percentage of Correct Predictions: 31.71%\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "N_ep = 100\n",
    "N_batch = 8\n",
    "\n",
    "main_path = os.getcwd()\n",
    "errts_path = '../../preprocess/errts'\n",
    "classifier_type = 'within_comp'\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "folder_list = [folder for folder in os.listdir() if folder.startswith('s') and os.path.isdir(folder)]\n",
    "\n",
    "for folder in folder_list:\n",
    "    os.chdir(folder)\n",
    "    path_to_main = '../../'\n",
    "    full_errts_path = path_to_main + errts_path + '/' + folder + '/'\n",
    "    main(classifier_type, path_prefix=full_errts_path, epochs=N_ep, batch=N_batch)\n",
    "    \n",
    "    # Return to the parent directory\n",
    "    os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e460d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/1122_DL/Final/models/FC/between_comp'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4282fb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'between_comp'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaa05d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'between_comp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'between_comp'"
     ]
    }
   ],
   "source": [
    "os.chdir(classifier_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b449e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_240319",
   "language": "python",
   "name": "dl_240319"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
