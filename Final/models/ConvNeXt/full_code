import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import nibabel as nib
import random
import datetime
class MRIDataset(Dataset):
    def __init__(self, file_path):
        with open(file_path, 'r') as file:
            self.files = [line.strip() for line in file.readlines()]
        random.shuffle(self.files)  # Shuffle the list of files

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        img_path = self.files[idx]
        img = nib.load(img_path).get_fdata()
        img = np.float32(img)
        img = torch.from_numpy(img)
        if img.ndim == 4 and img.shape[-1] == 1:
            img = img.squeeze(-1)
        img = img.unsqueeze(0)  # Ensure channel dimension is present
        return img
class ConvNeXtT(nn.Module):
    def __init__(self):
        super(ConvNeXtT, self).__init__()
        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)
        self.ln1 = nn.LayerNorm([16])  # Adjust the size based on output shape
        self.gelu = nn.GELU()

        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)
        self.ln2 = nn.LayerNorm([32])  # Adjust size based on output

        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1)
        self.ln3 = nn.LayerNorm([64])  # Adjust size based on output

        self.adapool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(64, 10)  # Output layer; adjust the number of outputs as necessary

    def forward(self, x):
        # First block with skip connection
        identity = x  # Save input for skip connection
        x = self.conv1(x)
        x = self.ln1(x.permute(0, 2, 3, 4, 1)).permute(0, 4, 1, 2, 3)  # Normalize
        x = self.gelu(x)
        x += identity  # Add skip connection

        # Second block with skip connection
        identity = x  # Save input for skip connection
        x = self.conv2(x)
        x = self.ln2(x.permute(0, 2, 3, 4, 1)).permute(0, 4, 1, 2, 3)  # Normalize
        x = self.gelu(x)
        x += identity  # Add skip connection

        # Third block with skip connection
        identity = x  # Save input for skip connection
        x = self.conv3(x)
        x = self.ln3(x.permute(0, 2, 3, 4, 1)).permute(0, 4, 1, 2, 3)  # Normalize
        x = self.gelu(x)
        x += identity  # Add skip connection

        # Adaptive pooling and final fully connected layer
        x = self.adapool(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.fc(x)
        return x
class ConvNeXtAutoencoder(nn.Module):
    def __init__(self):
        super(ConvNeXtAutoencoder, self).__init__()
        
        # Encoder
        self.enc_conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)
        self.ln1 = nn.LayerNorm([16, 64, 76, 64])
        self.gelu = nn.GELU()
        self.enc_conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)
        self.ln2 = nn.LayerNorm([32, 32, 38, 32])
        self.enc_conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1)
        self.ln3 = nn.LayerNorm([64, 16, 19, 16])
        
        # Matching layers for skip connections
        self.match_conv1 = nn.Conv3d(1, 16, kernel_size=1, stride=1, padding=0)  # Matches enc_conv1 channels
        self.match_conv2 = nn.Conv3d(16, 32, kernel_size=1, stride=2, padding=0)  # Matches enc_conv2 channels and stride
        self.match_conv3 = nn.Conv3d(32, 64, kernel_size=1, stride=2, padding=0)  # Matches enc_conv3 channels and stride

        # Decoder
        self.dec_conv1 = nn.ConvTranspose3d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.ln4 = nn.LayerNorm([32, 32, 38, 32])
        self.dec_conv2 = nn.ConvTranspose3d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)
        self.ln5 = nn.LayerNorm([16, 64, 76, 64])
        self.dec_conv3 = nn.ConvTranspose3d(16, 1, kernel_size=3, stride=1, padding=1)
        self.ln6 = nn.LayerNorm([1, 64, 76, 64])

    def forward(self, x):
        # Encoder
        identity = self.match_conv1(x)  # Adjust identity dimensions to match output of enc_conv1
        x = self.enc_conv1(x)
        x = self.ln1(x)
        x = self.gelu(x)
        x += identity  # Skip connection

        identity = self.match_conv2(identity)  # Adjust identity dimensions to match output of enc_conv2
        x = self.enc_conv2(x)
        x = self.ln2(x)
        x = self.gelu(x)
        x += identity  # Skip connection

        identity = self.match_conv3(identity)  # Adjust identity dimensions to match output of enc_conv3
        x = self.enc_conv3(x)
        x = self.ln3(x)
        x = self.gelu(x)
        x += identity  # Skip connection

        # Decoder
        x = self.dec_conv1(x)
        x = self.ln4(x)
        x = self.gelu(x)

        x = self.dec_conv2(x)
        x = self.ln5(x)
        x = self.gelu(x)

        x = self.dec_conv3(x)
        x = self.ln6(x)
        x = self.gelu(x)

        return x
def train_autoencoder(model, dataloader, epochs, device, learning_rate=5e-2):
    criterion = torch.nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)
    
    current_utc = datetime.datetime.utcnow()
    gmt8_time = current_utc + datetime.timedelta(hours=8)
    current_time = gmt8_time.strftime("%Y-%m-%d %H:%M:%S")
    # current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    start_time = f'Start training at: {current_time}'
    print(start_time)
    
    model.train()

    current_time = gmt8_time.strftime("%Y-%m-%d_%H%M%S")
    with open(f'loss_info_{current_time}.txt', 'w') as f:  # Open the file in write mode to overwrite existing or create new
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            for data in dataloader:
                data = data.to(device)
                optimizer.zero_grad()
                outputs = model(data)
                loss = criterion(outputs, data)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
                num_batches += 1
            average_loss = total_loss / num_batches
            # Write the average loss to file and also print it
            scheduler.step(average_loss)
            
            # current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            current_utc = datetime.datetime.utcnow()
            gmt8_time = current_utc + datetime.timedelta(hours=8)
            current_time = gmt8_time.strftime("%Y-%m-%d %H:%M:%S")
            log_entry = f'Epoch {epoch+1}, Average Loss: {average_loss}, Timestamp: {current_time}'
            # Write the average loss and timestamp to file and also print it
            f.write(log_entry)
            print(log_entry)

            # Save the model at the end of training
            torch.save(model.state_dict(), 'ConvNeXtAutoencoder.pth')
            print('Model saved to ConvNeXtAutoencoder.pth')
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f'Using device: {device}')
    
    dataset = MRIDataset('event_list.txt')  # Update with actual path
    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
    
    model = ConvNeXtAutoencoder()
    
    model_path = 'ConvNeXtAutoencoder.pth'
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path))
        print('Loaded trained model.')
    else:
        print('No trained model found, starting training from scratch.')
    
    model = model.to(device)
    train_autoencoder(model, dataloader, epochs=40, device=device)
    

if __name__ == "__main__":
    main()