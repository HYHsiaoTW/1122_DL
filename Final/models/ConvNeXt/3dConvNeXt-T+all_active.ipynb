{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45535c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DL_240319/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random\n",
    "from pathlib import Path\n",
    "from timm.models.layers import trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3348da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, file_path, path_prefix=\"\"):\n",
    "        self.path_prefix = path_prefix\n",
    "        full_file_path = path_prefix + file_path\n",
    "        with open(full_file_path, 'r') as file:\n",
    "            data = [line.strip().split() for line in file.readlines()]\n",
    "        \n",
    "        self.sub_label = {sub_label: idx for idx, sub_label in enumerate(set(row[1] for row in data))}\n",
    "        self.cond_label = {cond_label: idx for idx, cond_label in enumerate(set(row[2] for row in data))}\n",
    "        self.act_label = {act_label: idx for idx, act_label in enumerate(set(row[3] for row in data))}\n",
    "        self.files = [(row[0], self.sub_label[row[1]], self.cond_label[row[2]], self.act_label[row[3]]) for row in data]\n",
    "        random.shuffle(self.files)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, sub_label, cond_label, act_label = self.files[idx]\n",
    "        full_img_path = self.path_prefix + img_path\n",
    "        \n",
    "        img = nib.load(full_img_path).get_fdata()\n",
    "        img = np.float32(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        if img.ndim == 4 and img.shape[-1] == 1:\n",
    "            img = img.squeeze(-1)\n",
    "        img = img.unsqueeze(0)\n",
    "        sub_label = torch.tensor(sub_label, dtype=torch.long)\n",
    "        cond_label = torch.tensor(cond_label, dtype=torch.long)\n",
    "        act_label = torch.tensor(act_label, dtype=torch.long)\n",
    "        return img, sub_label, cond_label, act_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01669c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        # Depthwise 3D convolution\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        # Layer normalization for 3D (adjusting for channel dimension)\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        # Pointwise convolutions using linear layers\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        # Layer scaling if it is utilized\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                  requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 4, 1)  # Permute to bring channel to last\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # Permute back to normal\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36bc10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b242307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self, in_chans=1, nc_sub=100, nc_cond=4, nc_act=4, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.):\n",
    "        super().__init__()\n",
    "        # Initial downsampling\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv3d(in_chans, dims[0], kernel_size=5, stride=3),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv3d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final normalization\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(dims[-1], 4096),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.head_sub = nn.Linear(4096, nc_sub)\n",
    "        self.head_cond = nn.Linear(4096, nc_cond)\n",
    "        self.head_act = nn.Linear(4096, nc_act)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head_sub.weight.data.mul_(head_init_scale)\n",
    "        self.head_sub.bias.data.mul_(head_init_scale)\n",
    "        self.head_cond.weight.data.mul_(head_init_scale)\n",
    "        self.head_cond.bias.data.mul_(head_init_scale)\n",
    "        self.head_act.weight.data.mul_(head_init_scale)\n",
    "        self.head_act.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv3d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-3, -2, -1]))  # global average pooling over spatial dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        sub_output = self.head_sub(x)\n",
    "        cond_output = self.head_cond(x)\n",
    "        act_output = self.head_act(x)\n",
    "        return sub_output, cond_output, act_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00428464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    print(f'Epoch {epoch + 1}, start')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_log_loss = 0.0\n",
    "    batch_idx = 0\n",
    "    for img, sub, cond, act in train_loader:\n",
    "        batch_idx += 1\n",
    "        img = img.to(device)\n",
    "        sub = sub.to(device)\n",
    "        cond = cond.to(device)\n",
    "        act = act.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        sub_o, cond_o, act_o = model(img)\n",
    "        loss_sub = criterion(sub_o, sub)\n",
    "        loss_cond = criterion(cond_o, cond)\n",
    "        loss_act = criterion(act_o, act)\n",
    "        loss = loss_sub + loss_cond + loss_act\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_log_loss += loss.item()\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            current_utc = datetime.datetime.utcnow()\n",
    "            gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "            current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            with open('training_log.txt', 'a') as log_file:\n",
    "                log_entry = (f'Epoch {epoch+1:02}, Batch {batch_idx+1:04}: '\n",
    "                             f'Train Loss: {running_log_loss / 10:.4f}, '\n",
    "                             f'Timestamp: {current_time}\\n')\n",
    "                log_file.write(log_entry)\n",
    "            running_log_loss = 0\n",
    "    \n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abdd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    cor_sub = 0\n",
    "    cor_cond = 0\n",
    "    cor_act = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, sub, cond, act in val_loader:\n",
    "            img = img.to(device)\n",
    "            sub = sub.to(device)\n",
    "            cond = cond.to(device)\n",
    "            act = act.to(device)\n",
    "            sub_o, cond_o, act_o = model(img)\n",
    "            loss_sub = criterion(sub_o, sub)\n",
    "            loss_cond = criterion(cond_o, cond)\n",
    "            loss_act = criterion(act_o, act)\n",
    "            loss = loss_sub + loss_cond + loss_act\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, pred_sub = sub_o.max(1)\n",
    "            _, pred_cond = cond_o.max(1)\n",
    "            _, pred_act = act_o.max(1)\n",
    "            cor_sub += pred_sub.eq(sub).sum().item()\n",
    "            cor_cond += pred_cond.eq(cond).sum().item()\n",
    "            cor_act += pred_act.eq(act).sum().item()\n",
    "            total += sub.size(0)\n",
    "            \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    acc_sub = 100. * cor_sub / total\n",
    "    acc_cond = 100. * cor_cond / total\n",
    "    acc_act = 100. * cor_act / total\n",
    "    \n",
    "    with open('validation_log.txt', 'a') as log_file:\n",
    "        current_utc = datetime.datetime.utcnow()\n",
    "        gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "        current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = (f'Epoch {epoch+1:03}, Val Loss: {val_loss:.4f}, Val ACC sub: {acc_sub:.2f}%, '\n",
    "                     f'Val ACC cond: {acc_cond:.2f}%, Val ACC act: {acc_act:.2f}%, Timestamp: {current_time}\\n')\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "    return val_loss, acc_sub, acc_cond, acc_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name, path_prefix=\"\", epochs=10, lr=0.001, batch_size = 4):\n",
    "    # Configuration and Hyperparameters\n",
    "    batch_size = batch_size\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_set_file = f'{model_name}+train.txt'\n",
    "    val_set_file = f'{model_name}+test.txt'\n",
    "    \n",
    "    train_dataset = MRIDataset(train_set_file, path_prefix=path_prefix)\n",
    "    val_dataset = MRIDataset(val_set_file, path_prefix=path_prefix)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    nc_sub = len(set(train_dataset.sub_label))\n",
    "    nc_cond = len(set(train_dataset.cond_label))\n",
    "    nc_act = len(set(train_dataset.act_label))\n",
    "    grand_results = []\n",
    "    \n",
    "    with open('training_log.txt', 'w') as f:\n",
    "        f.write(\"\")  # This clears the training log\n",
    "    with open('validation_log.txt', 'w') as f:\n",
    "        f.write(\"\")  # This clears the validation log\n",
    "    \n",
    "    model = ConvNeXt(in_chans=1, nc_sub=nc_sub, nc_cond=nc_cond, nc_act=nc_act)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    current_utc = datetime.datetime.utcnow()\n",
    "    gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "    current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start_time = f'Start training at: {current_time}'\n",
    "    print(start_time)\n",
    "    \n",
    "    checkpoints = glob.glob(f'{model_name}_epoch*.pth')\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print(f'Loading checkpoint: {latest_checkpoint}')\n",
    "        model.load_state_dict(torch.load(latest_checkpoint))\n",
    "        epoch_number = int(latest_checkpoint.split('_epoch')[1].split('.pth')[0])\n",
    "        start_epoch = epoch_number\n",
    "    else:\n",
    "        print('No checkpoint found, starting training from scratch.')\n",
    "        start_epoch = 0\n",
    "    \n",
    "    # Training and Validation Loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train_loss = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "        \n",
    "        val_loss, val_acc_sub, val_acc_cond, val_acc_act = \\\n",
    "        validate(model, device, val_loader, criterion, epoch)\n",
    "        \n",
    "        current_utc = datetime.datetime.utcnow()\n",
    "        gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "        current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        start_time = f'Start training at: {current_time}'\n",
    "        print(f'Epoch {epoch+1:03}, Train Loss: {train_loss:.4f}, Timestamp: {current_time},\\n'\n",
    "              f'Val Loss: {val_loss:.4f}, Val ACC sub: {val_acc_sub:.2f}%, \\n'\n",
    "              f'Val ACC cond: {val_acc_cond:.2f}%, Val ACC act: {val_acc_act:.2f}%')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'{model_name}_epoch{epoch+1:03}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c646fc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'full' already exists.\n",
      "Start training at: 2024-06-12 17:50:34\n",
      "No checkpoint found, starting training from scratch.\n",
      "Epoch 1, start\n",
      "Epoch 001, Train Loss: 5.7193, Timestamp: 2024-06-12 17:56:42,\n",
      "Val Loss: 5.5841, Val ACC sub: 2.36%, \n",
      "Val ACC cond: 48.11%, Val ACC act: 60.38%\n",
      "Epoch 2, start\n",
      "Epoch 002, Train Loss: 5.3800, Timestamp: 2024-06-12 18:02:19,\n",
      "Val Loss: 5.5179, Val ACC sub: 2.36%, \n",
      "Val ACC cond: 45.75%, Val ACC act: 60.38%\n",
      "Epoch 3, start\n",
      "Epoch 003, Train Loss: 5.2455, Timestamp: 2024-06-12 18:07:55,\n",
      "Val Loss: 5.4190, Val ACC sub: 1.89%, \n",
      "Val ACC cond: 40.57%, Val ACC act: 60.38%\n",
      "Epoch 4, start\n",
      "Epoch 004, Train Loss: 5.1019, Timestamp: 2024-06-12 18:13:28,\n",
      "Val Loss: 5.2238, Val ACC sub: 4.72%, \n",
      "Val ACC cond: 50.00%, Val ACC act: 60.38%\n",
      "Epoch 5, start\n",
      "Epoch 005, Train Loss: 4.9570, Timestamp: 2024-06-12 18:19:01,\n",
      "Val Loss: 5.0289, Val ACC sub: 7.55%, \n",
      "Val ACC cond: 52.36%, Val ACC act: 60.38%\n",
      "Epoch 6, start\n",
      "Epoch 006, Train Loss: 4.7781, Timestamp: 2024-06-12 18:24:33,\n",
      "Val Loss: 4.8158, Val ACC sub: 6.13%, \n",
      "Val ACC cond: 55.19%, Val ACC act: 60.38%\n",
      "Epoch 7, start\n",
      "Epoch 007, Train Loss: 4.6084, Timestamp: 2024-06-12 18:30:03,\n",
      "Val Loss: 4.7962, Val ACC sub: 7.55%, \n",
      "Val ACC cond: 53.30%, Val ACC act: 60.38%\n",
      "Epoch 8, start\n",
      "Epoch 008, Train Loss: 4.6499, Timestamp: 2024-06-12 18:35:20,\n",
      "Val Loss: 4.9817, Val ACC sub: 6.60%, \n",
      "Val ACC cond: 48.58%, Val ACC act: 60.38%\n",
      "Epoch 9, start\n",
      "Epoch 009, Train Loss: 4.5048, Timestamp: 2024-06-12 18:40:38,\n",
      "Val Loss: 5.0001, Val ACC sub: 6.13%, \n",
      "Val ACC cond: 45.28%, Val ACC act: 60.38%\n",
      "Epoch 10, start\n",
      "Epoch 010, Train Loss: 4.3457, Timestamp: 2024-06-12 18:45:53,\n",
      "Val Loss: 4.8708, Val ACC sub: 12.26%, \n",
      "Val ACC cond: 47.17%, Val ACC act: 60.38%\n",
      "Epoch 11, start\n",
      "Epoch 011, Train Loss: 4.1476, Timestamp: 2024-06-12 18:51:16,\n",
      "Val Loss: 4.6509, Val ACC sub: 12.26%, \n",
      "Val ACC cond: 55.19%, Val ACC act: 60.38%\n",
      "Epoch 12, start\n",
      "Epoch 012, Train Loss: 3.9399, Timestamp: 2024-06-12 18:56:38,\n",
      "Val Loss: 4.5229, Val ACC sub: 13.21%, \n",
      "Val ACC cond: 55.19%, Val ACC act: 60.38%\n",
      "Epoch 13, start\n",
      "Epoch 013, Train Loss: 3.7360, Timestamp: 2024-06-12 19:01:58,\n",
      "Val Loss: 4.4958, Val ACC sub: 16.51%, \n",
      "Val ACC cond: 55.66%, Val ACC act: 60.38%\n",
      "Epoch 14, start\n",
      "Epoch 014, Train Loss: 3.4849, Timestamp: 2024-06-12 19:07:21,\n",
      "Val Loss: 4.1887, Val ACC sub: 26.42%, \n",
      "Val ACC cond: 61.32%, Val ACC act: 58.96%\n",
      "Epoch 15, start\n",
      "Epoch 015, Train Loss: 3.2396, Timestamp: 2024-06-12 19:12:48,\n",
      "Val Loss: 4.3548, Val ACC sub: 27.36%, \n",
      "Val ACC cond: 54.72%, Val ACC act: 60.38%\n",
      "Epoch 16, start\n",
      "Epoch 016, Train Loss: 3.0316, Timestamp: 2024-06-12 19:18:11,\n",
      "Val Loss: 4.2844, Val ACC sub: 30.19%, \n",
      "Val ACC cond: 50.47%, Val ACC act: 60.38%\n",
      "Epoch 17, start\n",
      "Epoch 017, Train Loss: 2.6890, Timestamp: 2024-06-12 19:23:30,\n",
      "Val Loss: 4.2654, Val ACC sub: 35.85%, \n",
      "Val ACC cond: 57.55%, Val ACC act: 58.96%\n",
      "Epoch 18, start\n",
      "Epoch 018, Train Loss: 2.4024, Timestamp: 2024-06-12 19:28:51,\n",
      "Val Loss: 4.4736, Val ACC sub: 34.43%, \n",
      "Val ACC cond: 55.66%, Val ACC act: 59.43%\n",
      "Epoch 19, start\n",
      "Epoch 019, Train Loss: 2.1601, Timestamp: 2024-06-12 19:34:15,\n",
      "Val Loss: 4.4299, Val ACC sub: 38.68%, \n",
      "Val ACC cond: 53.30%, Val ACC act: 60.85%\n",
      "Epoch 20, start\n",
      "Epoch 020, Train Loss: 1.9818, Timestamp: 2024-06-12 19:39:38,\n",
      "Val Loss: 4.4695, Val ACC sub: 40.09%, \n",
      "Val ACC cond: 59.43%, Val ACC act: 58.96%\n",
      "Epoch 21, start\n",
      "Epoch 021, Train Loss: 1.7517, Timestamp: 2024-06-12 19:45:14,\n",
      "Val Loss: 4.5702, Val ACC sub: 41.51%, \n",
      "Val ACC cond: 63.21%, Val ACC act: 59.43%\n",
      "Epoch 22, start\n",
      "Epoch 022, Train Loss: 1.4880, Timestamp: 2024-06-12 19:51:14,\n",
      "Val Loss: 4.3679, Val ACC sub: 45.75%, \n",
      "Val ACC cond: 61.32%, Val ACC act: 59.43%\n",
      "Epoch 23, start\n",
      "Epoch 023, Train Loss: 1.3084, Timestamp: 2024-06-12 19:56:57,\n",
      "Val Loss: 4.2112, Val ACC sub: 44.81%, \n",
      "Val ACC cond: 66.98%, Val ACC act: 58.96%\n",
      "Epoch 24, start\n",
      "Epoch 024, Train Loss: 1.0587, Timestamp: 2024-06-12 20:02:31,\n",
      "Val Loss: 5.1817, Val ACC sub: 41.98%, \n",
      "Val ACC cond: 69.34%, Val ACC act: 57.55%\n",
      "Epoch 25, start\n",
      "Epoch 025, Train Loss: 0.9050, Timestamp: 2024-06-12 20:08:10,\n",
      "Val Loss: 4.9950, Val ACC sub: 46.23%, \n",
      "Val ACC cond: 67.45%, Val ACC act: 61.32%\n",
      "Epoch 26, start\n",
      "Epoch 026, Train Loss: 0.7831, Timestamp: 2024-06-12 20:13:56,\n",
      "Val Loss: 5.2255, Val ACC sub: 46.70%, \n",
      "Val ACC cond: 70.75%, Val ACC act: 57.08%\n",
      "Epoch 27, start\n",
      "Epoch 027, Train Loss: 0.6127, Timestamp: 2024-06-12 20:19:53,\n",
      "Val Loss: 5.5979, Val ACC sub: 51.42%, \n",
      "Val ACC cond: 71.23%, Val ACC act: 57.08%\n",
      "Epoch 28, start\n",
      "Epoch 028, Train Loss: 0.5787, Timestamp: 2024-06-12 20:25:46,\n",
      "Val Loss: 5.8857, Val ACC sub: 49.06%, \n",
      "Val ACC cond: 71.70%, Val ACC act: 59.91%\n",
      "Epoch 29, start\n",
      "Epoch 029, Train Loss: 0.4754, Timestamp: 2024-06-12 20:31:35,\n",
      "Val Loss: 6.2158, Val ACC sub: 49.53%, \n",
      "Val ACC cond: 72.17%, Val ACC act: 59.43%\n",
      "Epoch 30, start\n",
      "Epoch 030, Train Loss: 0.4425, Timestamp: 2024-06-12 20:37:26,\n",
      "Val Loss: 6.3749, Val ACC sub: 51.42%, \n",
      "Val ACC cond: 66.98%, Val ACC act: 57.55%\n",
      "Epoch 31, start\n",
      "Epoch 031, Train Loss: 0.4219, Timestamp: 2024-06-12 20:42:57,\n",
      "Val Loss: 6.3313, Val ACC sub: 49.06%, \n",
      "Val ACC cond: 71.23%, Val ACC act: 63.21%\n",
      "Epoch 32, start\n",
      "Epoch 032, Train Loss: 0.3446, Timestamp: 2024-06-12 20:48:32,\n",
      "Val Loss: 6.5411, Val ACC sub: 50.47%, \n",
      "Val ACC cond: 73.11%, Val ACC act: 55.66%\n",
      "Epoch 33, start\n",
      "Epoch 033, Train Loss: 0.3007, Timestamp: 2024-06-12 20:54:28,\n",
      "Val Loss: 7.5565, Val ACC sub: 48.58%, \n",
      "Val ACC cond: 70.28%, Val ACC act: 59.43%\n",
      "Epoch 34, start\n",
      "Epoch 034, Train Loss: 0.3277, Timestamp: 2024-06-12 21:00:13,\n",
      "Val Loss: 6.9854, Val ACC sub: 53.77%, \n",
      "Val ACC cond: 73.58%, Val ACC act: 61.32%\n",
      "Epoch 35, start\n",
      "Epoch 035, Train Loss: 0.2709, Timestamp: 2024-06-12 21:05:55,\n",
      "Val Loss: 8.0508, Val ACC sub: 49.06%, \n",
      "Val ACC cond: 71.23%, Val ACC act: 58.02%\n",
      "Epoch 36, start\n",
      "Epoch 036, Train Loss: 0.2997, Timestamp: 2024-06-12 21:11:34,\n",
      "Val Loss: 8.0301, Val ACC sub: 49.53%, \n",
      "Val ACC cond: 71.70%, Val ACC act: 58.96%\n",
      "Epoch 37, start\n",
      "Epoch 037, Train Loss: 0.3218, Timestamp: 2024-06-12 21:17:16,\n",
      "Val Loss: 7.8163, Val ACC sub: 48.11%, \n",
      "Val ACC cond: 70.75%, Val ACC act: 59.43%\n",
      "Epoch 38, start\n",
      "Epoch 038, Train Loss: 0.2293, Timestamp: 2024-06-12 21:23:03,\n",
      "Val Loss: 8.3306, Val ACC sub: 49.53%, \n",
      "Val ACC cond: 71.23%, Val ACC act: 58.49%\n",
      "Epoch 39, start\n",
      "Epoch 039, Train Loss: 0.2190, Timestamp: 2024-06-12 21:28:52,\n",
      "Val Loss: 7.8085, Val ACC sub: 53.77%, \n",
      "Val ACC cond: 70.75%, Val ACC act: 59.43%\n",
      "Epoch 40, start\n",
      "Epoch 040, Train Loss: 0.2465, Timestamp: 2024-06-12 21:34:30,\n",
      "Val Loss: 7.5600, Val ACC sub: 53.30%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 58.96%\n",
      "Epoch 41, start\n",
      "Epoch 041, Train Loss: 0.1963, Timestamp: 2024-06-12 21:40:03,\n",
      "Val Loss: 8.4599, Val ACC sub: 53.30%, \n",
      "Val ACC cond: 69.34%, Val ACC act: 59.91%\n",
      "Epoch 42, start\n",
      "Epoch 042, Train Loss: 0.2369, Timestamp: 2024-06-12 21:45:46,\n",
      "Val Loss: 8.1291, Val ACC sub: 52.83%, \n",
      "Val ACC cond: 72.17%, Val ACC act: 62.26%\n",
      "Epoch 43, start\n",
      "Epoch 043, Train Loss: 0.2997, Timestamp: 2024-06-12 21:51:34,\n",
      "Val Loss: 7.4148, Val ACC sub: 50.47%, \n",
      "Val ACC cond: 71.23%, Val ACC act: 61.32%\n",
      "Epoch 44, start\n",
      "Epoch 044, Train Loss: 0.2535, Timestamp: 2024-06-12 21:57:25,\n",
      "Val Loss: 7.5838, Val ACC sub: 53.30%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 59.91%\n",
      "Epoch 45, start\n",
      "Epoch 045, Train Loss: 0.1868, Timestamp: 2024-06-12 22:03:20,\n",
      "Val Loss: 7.8972, Val ACC sub: 52.36%, \n",
      "Val ACC cond: 72.17%, Val ACC act: 62.74%\n",
      "Epoch 46, start\n",
      "Epoch 046, Train Loss: 0.2121, Timestamp: 2024-06-12 22:09:02,\n",
      "Val Loss: 8.1420, Val ACC sub: 52.83%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 62.74%\n",
      "Epoch 47, start\n",
      "Epoch 047, Train Loss: 0.1866, Timestamp: 2024-06-12 22:14:45,\n",
      "Val Loss: 8.0087, Val ACC sub: 56.60%, \n",
      "Val ACC cond: 73.58%, Val ACC act: 59.43%\n",
      "Epoch 48, start\n",
      "Epoch 048, Train Loss: 0.1996, Timestamp: 2024-06-12 22:20:16,\n",
      "Val Loss: 8.0142, Val ACC sub: 57.08%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 59.91%\n",
      "Epoch 49, start\n",
      "Epoch 049, Train Loss: 0.2333, Timestamp: 2024-06-12 22:25:44,\n",
      "Val Loss: 7.6842, Val ACC sub: 54.25%, \n",
      "Val ACC cond: 75.47%, Val ACC act: 65.57%\n",
      "Epoch 50, start\n",
      "Epoch 050, Train Loss: 0.1883, Timestamp: 2024-06-12 22:31:19,\n",
      "Val Loss: 7.6168, Val ACC sub: 56.13%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 60.38%\n",
      "Epoch 51, start\n",
      "Epoch 051, Train Loss: 0.1466, Timestamp: 2024-06-12 22:37:03,\n",
      "Val Loss: 8.9546, Val ACC sub: 53.30%, \n",
      "Val ACC cond: 77.36%, Val ACC act: 58.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, start\n",
      "Epoch 052, Train Loss: 0.1898, Timestamp: 2024-06-12 22:42:27,\n",
      "Val Loss: 8.2440, Val ACC sub: 55.19%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 58.49%\n",
      "Epoch 53, start\n",
      "Epoch 053, Train Loss: 0.1889, Timestamp: 2024-06-12 22:47:53,\n",
      "Val Loss: 8.2767, Val ACC sub: 50.00%, \n",
      "Val ACC cond: 69.34%, Val ACC act: 59.43%\n",
      "Epoch 54, start\n",
      "Epoch 054, Train Loss: 0.1446, Timestamp: 2024-06-12 22:53:31,\n",
      "Val Loss: 8.0991, Val ACC sub: 54.25%, \n",
      "Val ACC cond: 75.00%, Val ACC act: 61.79%\n",
      "Epoch 55, start\n",
      "Epoch 055, Train Loss: 0.0840, Timestamp: 2024-06-12 22:59:18,\n",
      "Val Loss: 8.9339, Val ACC sub: 58.96%, \n",
      "Val ACC cond: 75.94%, Val ACC act: 61.32%\n",
      "Epoch 56, start\n",
      "Epoch 056, Train Loss: 0.1422, Timestamp: 2024-06-12 23:05:06,\n",
      "Val Loss: 9.3001, Val ACC sub: 52.83%, \n",
      "Val ACC cond: 71.23%, Val ACC act: 58.96%\n",
      "Epoch 57, start\n",
      "Epoch 057, Train Loss: 0.2600, Timestamp: 2024-06-12 23:10:40,\n",
      "Val Loss: 7.3882, Val ACC sub: 50.00%, \n",
      "Val ACC cond: 75.00%, Val ACC act: 62.26%\n",
      "Epoch 58, start\n",
      "Epoch 058, Train Loss: 0.1745, Timestamp: 2024-06-12 23:16:16,\n",
      "Val Loss: 8.0846, Val ACC sub: 56.60%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 59.91%\n",
      "Epoch 59, start\n",
      "Epoch 059, Train Loss: 0.1205, Timestamp: 2024-06-12 23:22:04,\n",
      "Val Loss: 8.0406, Val ACC sub: 57.08%, \n",
      "Val ACC cond: 75.94%, Val ACC act: 58.49%\n",
      "Epoch 60, start\n",
      "Epoch 060, Train Loss: 0.0819, Timestamp: 2024-06-12 23:27:58,\n",
      "Val Loss: 9.2042, Val ACC sub: 60.38%, \n",
      "Val ACC cond: 73.58%, Val ACC act: 59.91%\n",
      "Epoch 61, start\n",
      "Epoch 061, Train Loss: 0.1261, Timestamp: 2024-06-12 23:33:39,\n",
      "Val Loss: 8.4339, Val ACC sub: 56.13%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 61.79%\n",
      "Epoch 62, start\n",
      "Epoch 062, Train Loss: 0.1487, Timestamp: 2024-06-12 23:39:17,\n",
      "Val Loss: 8.7658, Val ACC sub: 54.72%, \n",
      "Val ACC cond: 75.47%, Val ACC act: 61.32%\n",
      "Epoch 63, start\n",
      "Epoch 063, Train Loss: 0.1431, Timestamp: 2024-06-12 23:44:50,\n",
      "Val Loss: 7.9253, Val ACC sub: 57.55%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 64.62%\n",
      "Epoch 64, start\n",
      "Epoch 064, Train Loss: 0.1552, Timestamp: 2024-06-12 23:50:25,\n",
      "Val Loss: 8.5639, Val ACC sub: 53.30%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 61.79%\n",
      "Epoch 65, start\n",
      "Epoch 065, Train Loss: 0.2122, Timestamp: 2024-06-12 23:55:47,\n",
      "Val Loss: 8.0122, Val ACC sub: 53.30%, \n",
      "Val ACC cond: 75.00%, Val ACC act: 63.68%\n",
      "Epoch 66, start\n",
      "Epoch 066, Train Loss: 0.1064, Timestamp: 2024-06-13 00:00:47,\n",
      "Val Loss: 7.6840, Val ACC sub: 60.38%, \n",
      "Val ACC cond: 76.89%, Val ACC act: 62.26%\n",
      "Epoch 67, start\n",
      "Epoch 067, Train Loss: 0.0760, Timestamp: 2024-06-13 00:05:37,\n",
      "Val Loss: 8.7773, Val ACC sub: 57.55%, \n",
      "Val ACC cond: 73.58%, Val ACC act: 62.74%\n",
      "Epoch 68, start\n",
      "Epoch 068, Train Loss: 0.0988, Timestamp: 2024-06-13 00:10:03,\n",
      "Val Loss: 9.5081, Val ACC sub: 54.72%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 61.79%\n",
      "Epoch 69, start\n",
      "Epoch 069, Train Loss: 0.1338, Timestamp: 2024-06-13 00:14:38,\n",
      "Val Loss: 8.3110, Val ACC sub: 58.49%, \n",
      "Val ACC cond: 74.06%, Val ACC act: 60.38%\n",
      "Epoch 70, start\n",
      "Epoch 070, Train Loss: 0.1314, Timestamp: 2024-06-13 00:19:43,\n",
      "Val Loss: 9.0486, Val ACC sub: 52.83%, \n",
      "Val ACC cond: 74.06%, Val ACC act: 58.02%\n",
      "Epoch 71, start\n",
      "Epoch 071, Train Loss: 0.1396, Timestamp: 2024-06-13 00:24:26,\n",
      "Val Loss: 8.6476, Val ACC sub: 57.55%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 60.85%\n",
      "Epoch 72, start\n",
      "Epoch 072, Train Loss: 0.1084, Timestamp: 2024-06-13 00:29:13,\n",
      "Val Loss: 8.7090, Val ACC sub: 56.60%, \n",
      "Val ACC cond: 77.83%, Val ACC act: 62.74%\n",
      "Epoch 73, start\n",
      "Epoch 073, Train Loss: 0.1445, Timestamp: 2024-06-13 00:34:02,\n",
      "Val Loss: 8.2400, Val ACC sub: 57.08%, \n",
      "Val ACC cond: 75.47%, Val ACC act: 63.68%\n",
      "Epoch 74, start\n",
      "Epoch 074, Train Loss: 0.1310, Timestamp: 2024-06-13 00:38:51,\n",
      "Val Loss: 8.4340, Val ACC sub: 59.91%, \n",
      "Val ACC cond: 73.58%, Val ACC act: 60.38%\n",
      "Epoch 75, start\n",
      "Epoch 075, Train Loss: 0.0925, Timestamp: 2024-06-13 00:43:42,\n",
      "Val Loss: 9.2860, Val ACC sub: 58.02%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 61.32%\n",
      "Epoch 76, start\n",
      "Epoch 076, Train Loss: 0.0973, Timestamp: 2024-06-13 00:48:32,\n",
      "Val Loss: 9.4147, Val ACC sub: 58.49%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 61.79%\n",
      "Epoch 77, start\n",
      "Epoch 077, Train Loss: 0.0973, Timestamp: 2024-06-13 00:53:23,\n",
      "Val Loss: 9.4408, Val ACC sub: 57.55%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 61.32%\n",
      "Epoch 78, start\n",
      "Epoch 078, Train Loss: 0.0792, Timestamp: 2024-06-13 00:58:15,\n",
      "Val Loss: 9.3896, Val ACC sub: 58.96%, \n",
      "Val ACC cond: 72.17%, Val ACC act: 62.26%\n",
      "Epoch 79, start\n",
      "Epoch 079, Train Loss: 0.0694, Timestamp: 2024-06-13 01:03:03,\n",
      "Val Loss: 9.1833, Val ACC sub: 59.91%, \n",
      "Val ACC cond: 75.94%, Val ACC act: 61.79%\n",
      "Epoch 80, start\n",
      "Epoch 080, Train Loss: 0.1041, Timestamp: 2024-06-13 01:07:48,\n",
      "Val Loss: 9.5552, Val ACC sub: 47.64%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 61.79%\n",
      "Epoch 81, start\n",
      "Epoch 081, Train Loss: 0.2368, Timestamp: 2024-06-13 01:12:30,\n",
      "Val Loss: 7.9411, Val ACC sub: 58.96%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 58.49%\n",
      "Epoch 82, start\n",
      "Epoch 082, Train Loss: 0.1261, Timestamp: 2024-06-13 01:17:14,\n",
      "Val Loss: 8.3970, Val ACC sub: 62.74%, \n",
      "Val ACC cond: 77.36%, Val ACC act: 59.43%\n",
      "Epoch 83, start\n",
      "Epoch 083, Train Loss: 0.0660, Timestamp: 2024-06-13 01:22:00,\n",
      "Val Loss: 9.3051, Val ACC sub: 61.32%, \n",
      "Val ACC cond: 77.36%, Val ACC act: 60.85%\n",
      "Epoch 84, start\n",
      "Epoch 084, Train Loss: 0.0444, Timestamp: 2024-06-13 01:26:49,\n",
      "Val Loss: 8.8752, Val ACC sub: 60.85%, \n",
      "Val ACC cond: 79.25%, Val ACC act: 60.38%\n",
      "Epoch 85, start\n",
      "Epoch 085, Train Loss: 0.0654, Timestamp: 2024-06-13 01:31:38,\n",
      "Val Loss: 8.6739, Val ACC sub: 60.85%, \n",
      "Val ACC cond: 75.47%, Val ACC act: 58.02%\n",
      "Epoch 86, start\n",
      "Epoch 086, Train Loss: 0.0899, Timestamp: 2024-06-13 01:36:29,\n",
      "Val Loss: 10.0778, Val ACC sub: 54.25%, \n",
      "Val ACC cond: 74.53%, Val ACC act: 62.74%\n",
      "Epoch 87, start\n",
      "Epoch 087, Train Loss: 0.1358, Timestamp: 2024-06-13 01:41:20,\n",
      "Val Loss: 8.6928, Val ACC sub: 59.91%, \n",
      "Val ACC cond: 73.11%, Val ACC act: 60.38%\n",
      "Epoch 88, start\n",
      "Epoch 088, Train Loss: 0.1016, Timestamp: 2024-06-13 01:46:09,\n",
      "Val Loss: 8.4427, Val ACC sub: 58.49%, \n",
      "Val ACC cond: 76.89%, Val ACC act: 58.96%\n",
      "Epoch 89, start\n",
      "Epoch 089, Train Loss: 0.0795, Timestamp: 2024-06-13 01:50:54,\n",
      "Val Loss: 8.7503, Val ACC sub: 60.85%, \n",
      "Val ACC cond: 74.06%, Val ACC act: 58.02%\n",
      "Epoch 90, start\n",
      "Epoch 090, Train Loss: 0.0891, Timestamp: 2024-06-13 01:55:36,\n",
      "Val Loss: 9.0515, Val ACC sub: 64.15%, \n",
      "Val ACC cond: 72.64%, Val ACC act: 62.26%\n",
      "Epoch 91, start\n",
      "Epoch 091, Train Loss: 0.0931, Timestamp: 2024-06-13 02:00:13,\n",
      "Val Loss: 8.8220, Val ACC sub: 63.68%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 62.26%\n",
      "Epoch 92, start\n",
      "Epoch 092, Train Loss: 0.0941, Timestamp: 2024-06-13 02:05:01,\n",
      "Val Loss: 8.2694, Val ACC sub: 62.26%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 61.32%\n",
      "Epoch 93, start\n",
      "Epoch 093, Train Loss: 0.1093, Timestamp: 2024-06-13 02:09:47,\n",
      "Val Loss: 8.2799, Val ACC sub: 63.21%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 60.38%\n",
      "Epoch 94, start\n",
      "Epoch 094, Train Loss: 0.0954, Timestamp: 2024-06-13 02:14:36,\n",
      "Val Loss: 8.8116, Val ACC sub: 57.08%, \n",
      "Val ACC cond: 73.58%, Val ACC act: 59.91%\n",
      "Epoch 95, start\n",
      "Epoch 095, Train Loss: 0.0575, Timestamp: 2024-06-13 02:19:25,\n",
      "Val Loss: 8.2439, Val ACC sub: 65.09%, \n",
      "Val ACC cond: 75.47%, Val ACC act: 61.32%\n",
      "Epoch 96, start\n",
      "Epoch 096, Train Loss: 0.0318, Timestamp: 2024-06-13 02:24:15,\n",
      "Val Loss: 8.5787, Val ACC sub: 64.15%, \n",
      "Val ACC cond: 76.89%, Val ACC act: 61.79%\n",
      "Epoch 97, start\n",
      "Epoch 097, Train Loss: 0.0254, Timestamp: 2024-06-13 02:29:04,\n",
      "Val Loss: 9.4840, Val ACC sub: 61.32%, \n",
      "Val ACC cond: 74.06%, Val ACC act: 61.32%\n",
      "Epoch 98, start\n",
      "Epoch 098, Train Loss: 0.0763, Timestamp: 2024-06-13 02:33:50,\n",
      "Val Loss: 9.1270, Val ACC sub: 55.66%, \n",
      "Val ACC cond: 75.94%, Val ACC act: 60.38%\n",
      "Epoch 99, start\n",
      "Epoch 099, Train Loss: 0.1641, Timestamp: 2024-06-13 02:38:26,\n",
      "Val Loss: 7.7776, Val ACC sub: 62.26%, \n",
      "Val ACC cond: 79.72%, Val ACC act: 60.38%\n",
      "Epoch 100, start\n",
      "Epoch 100, Train Loss: 0.1371, Timestamp: 2024-06-13 02:43:06,\n",
      "Val Loss: 8.3807, Val ACC sub: 58.02%, \n",
      "Val ACC cond: 76.42%, Val ACC act: 61.32%\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "N_epoch = 100\n",
    "N_batch = 12\n",
    "lr = 0.001\n",
    "\n",
    "main_path = os.getcwd()\n",
    "\n",
    "os.chdir(main_path)\n",
    "classifier_type = 'full'\n",
    "\n",
    "try:\n",
    "    os.mkdir(classifier_type)\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{classifier_type}' already exists.\")\n",
    "\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "####################\n",
    "main(classifier_type, epochs=N_epoch, batch_size = N_batch, lr=lr)\n",
    "\n",
    "os.chdir(main_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac9344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'condition' already exists.\n",
      "Folder 's11A' already exists.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../preprocess/errts/s11A/condition+train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m path_to_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m full_errts_path \u001b[38;5;241m=\u001b[39m path_to_main \u001b[38;5;241m+\u001b[39m errts_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_errts_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_ep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Return to the parent directory\u001b[39;00m\n\u001b[1;32m     33\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_name, path_prefix, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m train_set_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m val_set_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMRIDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m MRIDataset(val_set_file, path_prefix\u001b[38;5;241m=\u001b[39mpath_prefix)\n\u001b[1;32m     12\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mMRIDataset.__init__\u001b[0;34m(self, file_path, path_prefix)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_prefix \u001b[38;5;241m=\u001b[39m path_prefix\n\u001b[1;32m      4\u001b[0m full_file_path \u001b[38;5;241m=\u001b[39m path_prefix \u001b[38;5;241m+\u001b[39m file_path\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfull_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_label \u001b[38;5;241m=\u001b[39m {sub_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, sub_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n",
      "File \u001b[0;32m/opt/conda/envs/DL_240319/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../preprocess/errts/s11A/condition+train.txt'"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "N_ep = 30\n",
    "\n",
    "main_path = os.getcwd()\n",
    "errts_path = '../../preprocess/errts'\n",
    "os.chdir(errts_path)\n",
    "\n",
    "folder_list = [folder for folder in os.listdir() if folder.startswith('s') and os.path.isdir(folder)]\n",
    "\n",
    "os.chdir(main_path)\n",
    "classifier_type = 'condition'\n",
    "\n",
    "try:\n",
    "    os.mkdir(classifier_type)\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{classifier_type}' already exists.\")\n",
    "\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "for folder in folder_list:\n",
    "    # Change to the directory\n",
    "    os.chdir(main_path)\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder}' already exists.\")\n",
    "    os.chdir(folder)\n",
    "    path_to_main = '../../'\n",
    "    full_errts_path = path_to_main + errts_path + '/' + folder + '/'\n",
    "    main(classifier_type, path_prefix=full_errts_path, epochs=N_ep)\n",
    "    \n",
    "    # Return to the parent directory\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf4ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_240319",
   "language": "python",
   "name": "dl_240319"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
