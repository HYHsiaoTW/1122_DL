{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa4aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### this is for classification in single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5e9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 08:48:30.784639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-13 08:48:30.784680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-13 08:48:30.785100: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-13 08:48:30.787982: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "import random\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec49ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, file_path, path_prefix=\"\"):\n",
    "        # Initialize with an optional path prefix\n",
    "        self.path_prefix = path_prefix\n",
    "        \n",
    "        full_file_path = path_prefix + file_path\n",
    "        with open(full_file_path, 'r') as file:\n",
    "            data = [line.strip().split() for line in file.readlines()]\n",
    "        \n",
    "        self.labels = {label: idx for idx, label in enumerate(set(row[1] for row in data))}\n",
    "        self.files = [(row[0], self.labels[row[1]]) for row in data]\n",
    "        random.shuffle(self.files)  # Shuffle the list of files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.files[idx]\n",
    "        full_img_path = self.path_prefix + img_path\n",
    "        img = nib.load(full_img_path).get_fdata()\n",
    "        img = np.float32(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        if img.ndim == 4 and img.shape[-1] == 1:\n",
    "            img = img.squeeze(-1)\n",
    "        img = img.unsqueeze(0)  # Ensure channel dimension is present\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc9f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtT(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(ConvNeXtT, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([16, 64, 76, 64])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([32, 32, 38, 32])\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln3 = nn.LayerNorm([64, 16, 19, 16])\n",
    "\n",
    "        self.adapool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)  # Output layer; adjust the number of outputs as necessary\n",
    "        \n",
    "        # Matching layers for skip connections\n",
    "        self.match_conv1 = nn.Conv3d(1, 16, kernel_size=1, stride=1, padding=0)  # Matches enc_conv1 channels\n",
    "        self.match_conv2 = nn.Conv3d(16, 32, kernel_size=1, stride=2, padding=0)  # Matches enc_conv2 channels and stride\n",
    "        self.match_conv3 = nn.Conv3d(32, 64, kernel_size=1, stride=2, padding=0)  # Matches enc_conv3 channels and stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block with skip connection\n",
    "        identity = self.match_conv1(x)  # Save input for skip connection\n",
    "        x = self.conv1(x)\n",
    "        x = self.ln1(x)  # Normalize\n",
    "        x = self.gelu(x)\n",
    "        x += identity  # Add skip connection\n",
    "\n",
    "        # Second block with skip connection\n",
    "        identity = self.match_conv2(x)  # Save input for skip connection\n",
    "        x = self.conv2(x)\n",
    "        x = self.ln2(x)  # Normalize\n",
    "        x = self.gelu(x)\n",
    "        x += identity  # Add skip connection\n",
    "\n",
    "        # Third block with skip connection\n",
    "        identity = self.match_conv3(x)  # Save input for skip connection\n",
    "        x = self.conv3(x)\n",
    "        x = self.ln3(x)  # Normalize\n",
    "        x = self.gelu(x)\n",
    "        x += identity  # Add skip connection\n",
    "\n",
    "        # Adaptive pooling and final fully connected layer\n",
    "        x = self.adapool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26de1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, test_data=None, epochs=10):\n",
    "    model.train()\n",
    "    with open('training_log.txt', 'a') as log_file:\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                labels = labels.to(device)\n",
    "                inputs = inputs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            average_loss = total_loss / num_batches\n",
    "            current_utc = datetime.datetime.utcnow()\n",
    "            gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "            current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            log_entry = f'Epoch {epoch+1:03}, Average Loss: {average_loss}, Timestamp: {current_time}\\n'\n",
    "            # Write the log entry to the file\n",
    "            log_file.write(log_entry)\n",
    "            # Test every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0 and test_data is not None:\n",
    "                test_input, test_label = test_data\n",
    "                test_result = test_model(model, test_input, test_label, device)\n",
    "                log_file.write(f\"Test at Epoch {epoch+1:03}: {test_result}\\n\")\n",
    "\n",
    "\n",
    "def test_model(model, test_input, test_label, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure test_input is a tensor and move it to the correct device\n",
    "    if not torch.is_tensor(test_input):\n",
    "        test_input = torch.tensor(test_input, dtype=torch.float, device=device)\n",
    "    else:\n",
    "        test_input = test_input.to(device)\n",
    "\n",
    "    # Ensure test_label is a tensor, add a batch dimension, and move to correct device\n",
    "    if isinstance(test_label, int):\n",
    "        test_label = torch.tensor([test_label], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Perform model inference and get the predicted class\n",
    "        outputs = model(test_input.unsqueeze(0))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Check if the prediction is correct\n",
    "        correct = (predicted == test_label).item()  # Convert the result to Python boolean\n",
    "\n",
    "    return \"Correct\" if correct else \"Incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56ab829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(classifier, path_prefix=\"\", epochs=10, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    dataset_file = f'{classifier}+classify.txt'\n",
    "    \n",
    "    full_dataset = MRIDataset(dataset_file, path_prefix=path_prefix)\n",
    "    num_classes = len(full_dataset.labels)\n",
    "    grand_results = []\n",
    "    \n",
    "    current_utc = datetime.datetime.utcnow()\n",
    "    gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "    current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start_time = f'Start training at: {current_time}'\n",
    "    print(start_time)\n",
    "    \n",
    "    for i in range(len(full_dataset)):\n",
    "        train_indices = list(range(len(full_dataset)))\n",
    "        train_indices.pop(i)  # Remove the test image index\n",
    "        test_index = i\n",
    "        \n",
    "        train_subset = Subset(full_dataset, train_indices)\n",
    "        test_input, test_label = full_dataset[test_index]\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=4, shuffle=True)\n",
    "\n",
    "        model = ConvNeXtT(num_classes)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "        train_model(model, train_loader, criterion, optimizer, device, test_data=full_dataset[test_index], epochs=epochs)\n",
    "        result = test_model(model, test_input, test_label, device)\n",
    "        grand_results.append(result)\n",
    "        \n",
    "        os.rename('training_log.txt', f'training_log+stim_{i+1:03}.txt')\n",
    "\n",
    "    with open(f'{classifier}_final_results.log', 'w') as f:\n",
    "        correct_count = grand_results.count(\"Correct\")\n",
    "        total_tests = len(grand_results)\n",
    "        correct_percentage = (correct_count / total_tests) * 100 if total_tests > 0 else 0\n",
    "        for idx, result in enumerate(grand_results):\n",
    "            f.write(f\"Model {idx+1:03}: Result: {result}\\n\")\n",
    "        f.write(f\"Percentage of Correct Predictions: {correct_percentage:.2f}%\\n\")\n",
    "\n",
    "    print(f\"Percentage of Correct Predictions: {correct_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5672babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Start training at: 2024-05-13 16:48:32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m path_to_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m full_errts_path \u001b[38;5;241m=\u001b[39m path_to_main \u001b[38;5;241m+\u001b[39m errts_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_errts_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_ep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Return to the parent directory\u001b[39;00m\n\u001b[1;32m     32\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(classifier, path_prefix, epochs, lr)\u001b[0m\n\u001b[1;32m     31\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     32\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m result \u001b[38;5;241m=\u001b[39m test_model(model, test_input, test_label, device)\n\u001b[1;32m     36\u001b[0m grand_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device, test_data, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "N_ep = 200\n",
    "\n",
    "main_path = os.getcwd()\n",
    "errts_path = '../../preprocess/errts'\n",
    "os.chdir(errts_path)\n",
    "\n",
    "folder_list = [folder for folder in os.listdir() if folder.startswith('s') and os.path.isdir(folder)]\n",
    "\n",
    "os.chdir(main_path)\n",
    "classifier_type = 'condition'\n",
    "\n",
    "try:\n",
    "    os.mkdir(classifier_type)\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{classifier_type}' already exists.\")\n",
    "\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "for folder in folder_list:\n",
    "    # Change to the directory\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder}' already exists.\")\n",
    "    os.chdir(folder)\n",
    "    path_to_main = '../../'\n",
    "    full_errts_path = path_to_main + errts_path + '/' + folder + '/'\n",
    "    main(classifier_type, path_prefix=full_errts_path, epochs=N_ep)\n",
    "    \n",
    "    # Return to the parent directory\n",
    "    os.chdir('..')\n",
    "\n",
    "os.chdir(main_path)\n",
    "classifier_type = 'truth_tell'\n",
    "\n",
    "try:\n",
    "    os.mkdir(classifier_type)\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{classifier_type}' already exists.\")\n",
    "\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "for folder in folder_list:\n",
    "    # Change to the directory\n",
    "    os.chdir(main_path)\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder}' already exists.\")\n",
    "    os.chdir(folder)\n",
    "    path_to_main = '../../'\n",
    "    full_errts_path = path_to_main + errts_path + '/' + folder + '/'\n",
    "    main(classifier_type, path_prefix=full_errts_path, epochs=N_ep)\n",
    "    \n",
    "    # Return to the parent directory\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e460d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_240319",
   "language": "python",
   "name": "dl_240319"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
