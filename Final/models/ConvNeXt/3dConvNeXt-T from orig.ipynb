{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45535c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DL_240319/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random\n",
    "from pathlib import Path\n",
    "from timm.models.layers import trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3348da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, file_path, path_prefix=\"\"):\n",
    "        self.path_prefix = path_prefix\n",
    "        full_file_path = path_prefix + file_path\n",
    "        with open(full_file_path, 'r') as file:\n",
    "            data = [line.strip().split() for line in file.readlines()]\n",
    "        \n",
    "        self.sub_label = {sub_label: idx for idx, sub_label in enumerate(set(row[1] for row in data))}\n",
    "        self.cond_label = {cond_label: idx for idx, cond_label in enumerate(set(row[2] for row in data))}\n",
    "        self.stg_label = {stg_label: idx for idx, stg_label in enumerate(set(row[3] for row in data))}\n",
    "        self.act_label = {act_label: idx for idx, act_label in enumerate(set(row[4] for row in data))}\n",
    "        self.out_label = {out_label: idx for idx, out_label in enumerate(set(row[5] for row in data))}\n",
    "        self.files = [(row[0], self.sub_label[row[1]], self.cond_label[row[2]], self.stg_label[row[3]],\n",
    "                       self.act_label[row[4]], self.out_label[row[5]]) for row in data]\n",
    "        random.shuffle(self.files)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, sub_label, cond_label, stg_label, act_label, out_label = self.files[idx]\n",
    "        full_img_path = self.path_prefix + img_path\n",
    "        \n",
    "        img = nib.load(full_img_path).get_fdata()\n",
    "        img = np.float32(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        if img.ndim == 4 and img.shape[-1] == 1:\n",
    "            img = img.squeeze(-1)\n",
    "        img = img.unsqueeze(0)\n",
    "        sub_label = torch.tensor(sub_label, dtype=torch.long)\n",
    "        cond_label = torch.tensor(cond_label, dtype=torch.long)\n",
    "        stg_label = torch.tensor(stg_label, dtype=torch.long)\n",
    "        act_label = torch.tensor(act_label, dtype=torch.long)\n",
    "        out_label = torch.tensor(out_label, dtype=torch.long)\n",
    "        return img, sub_label, cond_label, stg_label, act_label, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01669c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        # Depthwise 3D convolution\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        # Layer normalization for 3D (adjusting for channel dimension)\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        # Pointwise convolutions using linear layers\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        # Layer scaling if it is utilized\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                  requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 4, 1)  # Permute to bring channel to last\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # Permute back to normal\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36bc10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b242307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    def __init__(self, in_chans=1, nc_sub=100, nc_cond=4, nc_stg=4, nc_act=4, nc_out=4, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.):\n",
    "        super().__init__()\n",
    "        # Initial downsampling\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv3d(in_chans, dims[0], kernel_size=5, stride=3),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv3d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final normalization\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(dims[-1], 4096),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.head_sub = nn.Linear(4096, nc_sub)\n",
    "        self.head_cond = nn.Linear(4096, nc_cond)\n",
    "        self.head_stg = nn.Linear(4096, nc_stg)\n",
    "        self.head_act = nn.Linear(4096, nc_act)\n",
    "        self.head_out = nn.Linear(4096, nc_out)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head_sub.weight.data.mul_(head_init_scale)\n",
    "        self.head_sub.bias.data.mul_(head_init_scale)\n",
    "        self.head_cond.weight.data.mul_(head_init_scale)\n",
    "        self.head_cond.bias.data.mul_(head_init_scale)\n",
    "        self.head_stg.weight.data.mul_(head_init_scale)\n",
    "        self.head_stg.bias.data.mul_(head_init_scale)\n",
    "        self.head_act.weight.data.mul_(head_init_scale)\n",
    "        self.head_act.bias.data.mul_(head_init_scale)\n",
    "        self.head_out.weight.data.mul_(head_init_scale)\n",
    "        self.head_out.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv3d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-3, -2, -1]))  # global average pooling over spatial dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        sub_output = self.head_sub(x)\n",
    "        cond_output = self.head_cond(x)\n",
    "        stg_output = self.head_stg(x)\n",
    "        act_output = self.head_act(x)\n",
    "        out_output = self.head_out(x)\n",
    "        return sub_output, cond_output, stg_output, act_output, out_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00428464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    print(f'Epoch {epoch + 1}, start')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_log_loss = 0.0\n",
    "    batch_idx = 0\n",
    "    for img, sub, cond, stg, act, out in train_loader:\n",
    "        batch_idx += 1\n",
    "        img = img.to(device)\n",
    "        sub = sub.to(device)\n",
    "        cond = cond.to(device)\n",
    "        stg = stg.to(device)\n",
    "        act = act.to(device)\n",
    "        out = out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        sub_o, cond_o, stg_o, act_o, out_o = model(img)\n",
    "        loss_sub = criterion(sub_o, sub)\n",
    "        loss_cond = criterion(cond_o, cond)\n",
    "        loss_stg = criterion(stg_o, stg)\n",
    "        loss_act = criterion(act_o, act)\n",
    "        loss_out = criterion(out_o, out)\n",
    "        loss = loss_sub + loss_cond + loss_stg + loss_act + loss_out\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_log_loss += loss.item()\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            current_utc = datetime.datetime.utcnow()\n",
    "            gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "            current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            with open('training_log.txt', 'a') as log_file:\n",
    "                log_entry = (f'Epoch {epoch+1:02}, Batch {batch_idx+1:04}: '\n",
    "                             f'Train Loss: {running_log_loss / 10:.4f}, '\n",
    "                             f'Timestamp: {current_time}\\n')\n",
    "                log_file.write(log_entry)\n",
    "            running_log_loss = 0\n",
    "    \n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abdd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    cor_sub = 0\n",
    "    cor_cond = 0\n",
    "    cor_stg = 0\n",
    "    cor_act = 0\n",
    "    cor_out = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, sub, cond, stg, act, out in val_loader:\n",
    "            img = img.to(device)\n",
    "            sub = sub.to(device)\n",
    "            cond = cond.to(device)\n",
    "            stg = stg.to(device)\n",
    "            act = act.to(device)\n",
    "            out = out.to(device)\n",
    "            sub_o, cond_o, stg_o, act_o, out_o = model(img)\n",
    "            loss_sub = criterion(sub_o, sub)\n",
    "            loss_cond = criterion(cond_o, cond)\n",
    "            loss_stg = criterion(stg_o, stg)\n",
    "            loss_act = criterion(act_o, act)\n",
    "            loss_out = criterion(out_o, out)\n",
    "            loss = loss_sub + loss_cond + loss_stg + loss_act + loss_out\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, pred_sub = sub_o.max(1)\n",
    "            _, pred_cond = cond_o.max(1)\n",
    "            _, pred_stg = stg_o.max(1)\n",
    "            _, pred_act = act_o.max(1)\n",
    "            _, pred_out = out_o.max(1)\n",
    "            cor_sub += pred_sub.eq(sub).sum().item()\n",
    "            cor_cond += pred_cond.eq(cond).sum().item()\n",
    "            cor_stg += pred_stg.eq(stg).sum().item()\n",
    "            cor_act += pred_act.eq(act).sum().item()\n",
    "            cor_out += pred_out.eq(out).sum().item()\n",
    "            total += sub.size(0)\n",
    "            \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    acc_sub = 100. * cor_sub / total\n",
    "    acc_cond = 100. * cor_cond / total\n",
    "    acc_stg = 100. * cor_stg / total\n",
    "    acc_act = 100. * cor_act / total\n",
    "    acc_out = 100. * cor_out / total\n",
    "    \n",
    "    with open('validation_log.txt', 'a') as log_file:\n",
    "        current_utc = datetime.datetime.utcnow()\n",
    "        gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "        current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = (f'Epoch {epoch+1:03}, Val Loss: {val_loss:.4f}, Val ACC sub: {acc_sub:.2f}%, '\n",
    "                     f'Val ACC cond: {acc_cond:.2f}%, Val ACC stg: {acc_stg:.2f}%, Val ACC act: {acc_act:.2f}%, '\n",
    "                     f'Val ACC out: {acc_out:.2f}%, Timestamp: {current_time}\\n')\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "    return val_loss, acc_sub, acc_cond, acc_stg, acc_act, acc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name, path_prefix=\"\", epochs=10, lr=0.001, batch_size = 4):\n",
    "    # Configuration and Hyperparameters\n",
    "    batch_size = batch_size\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_set_file = f'{model_name}+train.txt'\n",
    "    val_set_file = f'{model_name}+val.txt'\n",
    "    \n",
    "    train_dataset = MRIDataset(train_set_file, path_prefix=path_prefix)\n",
    "    val_dataset = MRIDataset(val_set_file, path_prefix=path_prefix)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    nc_sub = len(train_dataset.sub_label)\n",
    "    nc_cond = len(train_dataset.cond_label)\n",
    "    nc_stg = len(train_dataset.stg_label)\n",
    "    nc_act = len(train_dataset.act_label)\n",
    "    nc_out = len(train_dataset.out_label)\n",
    "    grand_results = []\n",
    "    \n",
    "    with open('training_log.txt', 'w') as f:\n",
    "        f.write(\"\")  # This clears the training log\n",
    "    with open('validation_log.txt', 'w') as f:\n",
    "        f.write(\"\")  # This clears the validation log\n",
    "    \n",
    "    model = ConvNeXt(in_chans=1, nc_sub=nc_sub, nc_cond=nc_cond, nc_stg=nc_stg, nc_act=nc_act, nc_out=nc_out)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    current_utc = datetime.datetime.utcnow()\n",
    "    gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "    current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start_time = f'Start training at: {current_time}'\n",
    "    print(start_time)\n",
    "        \n",
    "    # Training and Validation Loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "        \n",
    "        val_loss, val_acc_sub, val_acc_cond, val_acc_stg, val_acc_act, val_acc_out = \\\n",
    "        validate(model, device, val_loader, criterion, epoch)\n",
    "        \n",
    "        current_utc = datetime.datetime.utcnow()\n",
    "        gmt8_time = current_utc + datetime.timedelta(hours=8)\n",
    "        current_time = gmt8_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        start_time = f'Start training at: {current_time}'\n",
    "        print(f'Epoch {epoch+1:03}, Train Loss: {train_loss:.4f}, Timestamp: {current_time},\\n'\n",
    "              f'Val Loss: {val_loss:.4f}, Val ACC sub: {val_acc_sub:.2f}%, \\n'\n",
    "              f'Val ACC cond: {val_acc_cond:.2f}%, Val ACC stg: {val_acc_stg:.2f}%, \\n'\n",
    "              f'Val ACC act: {val_acc_act:.2f}%, Val ACC out: {val_acc_out:.2f}%')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'{model_name}_epoch{epoch+1:02}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c646fc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'full' already exists.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(classifier_type)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(main_path)\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_name, path_prefix, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m train_set_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m val_set_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+val.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMRIDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m MRIDataset(val_set_file, path_prefix\u001b[38;5;241m=\u001b[39mpath_prefix)\n\u001b[1;32m     12\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mMRIDataset.__init__\u001b[0;34m(self, file_path, path_prefix)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_label \u001b[38;5;241m=\u001b[39m {cond_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, cond_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstg_label \u001b[38;5;241m=\u001b[39m {stg_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, stg_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_label \u001b[38;5;241m=\u001b[39m {act_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, act_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_label \u001b[38;5;241m=\u001b[39m {out_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, out_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [(row[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_label[row[\u001b[38;5;241m1\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_label[row[\u001b[38;5;241m2\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstg_label[row[\u001b[38;5;241m3\u001b[39m]],\n\u001b[1;32m     14\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_label[row[\u001b[38;5;241m4\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_label[row[\u001b[38;5;241m5\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_label \u001b[38;5;241m=\u001b[39m {cond_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, cond_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstg_label \u001b[38;5;241m=\u001b[39m {stg_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, stg_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_label \u001b[38;5;241m=\u001b[39m {act_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, act_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_label \u001b[38;5;241m=\u001b[39m {out_label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, out_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(row[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data))}\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [(row[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_label[row[\u001b[38;5;241m1\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_label[row[\u001b[38;5;241m2\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstg_label[row[\u001b[38;5;241m3\u001b[39m]],\n\u001b[1;32m     14\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_label[row[\u001b[38;5;241m4\u001b[39m]], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_label[row[\u001b[38;5;241m5\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "N_epoch = 30\n",
    "N_batch = 16\n",
    "\n",
    "main_path = os.getcwd()\n",
    "\n",
    "os.chdir(main_path)\n",
    "classifier_type = 'full'\n",
    "\n",
    "try:\n",
    "    os.mkdir(classifier_type)\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{classifier_type}' already exists.\")\n",
    "\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "####################\n",
    "main(classifier_type, epochs=N_epoch, batch_size = N_batch)\n",
    "\n",
    "os.chdir(main_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "N_ep = 30\n",
    "\n",
    "main_path = os.getcwd()\n",
    "errts_path = '../../preprocess/errts'\n",
    "os.chdir(errts_path)\n",
    "\n",
    "folder_list = [folder for folder in os.listdir() if folder.startswith('s') and os.path.isdir(folder)]\n",
    "\n",
    "os.chdir(main_path)\n",
    "classifier_type = 'condition'\n",
    "\n",
    "try:\n",
    "    os.mkdir(classifier_type)\n",
    "except FileExistsError:\n",
    "    print(f\"Folder '{classifier_type}' already exists.\")\n",
    "\n",
    "os.chdir(classifier_type)\n",
    "\n",
    "for folder in folder_list:\n",
    "    # Change to the directory\n",
    "    os.chdir(main_path)\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder}' already exists.\")\n",
    "    os.chdir(folder)\n",
    "    path_to_main = '../../'\n",
    "    full_errts_path = path_to_main + errts_path + '/' + folder + '/'\n",
    "    main(classifier_type, path_prefix=full_errts_path, epochs=N_ep)\n",
    "    \n",
    "    # Return to the parent directory\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf4ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_240319",
   "language": "python",
   "name": "dl_240319"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
